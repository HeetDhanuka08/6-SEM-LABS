{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "NAME: Heet Dhanuka\n",
        "\n",
        "ROLL NO.: B - 34\n",
        "\n",
        "BATCH: B2\n",
        "\n",
        "NLP PRACTICAL 2\n",
        "\n",
        "---\n",
        "\n",
        "AIM :- 1. Print the statistics of standard corpus using NLTK. check wheteher tagged or untagged corpus\n",
        "Gutenberg , Reuters , Webtexz , Genesis , Brown\n",
        "2. Access any document of webtext/ gutenberg corpus, perform word and sentence tokenization using nltk\n",
        "print the count of number of words and sentences in a doc\n",
        "3. Remove all stopwords and punctuation symbols and print the  unique words\n",
        "4. Perform POS tagging and stemming using porterstemmer on the unique list of words\n",
        "\n"
      ],
      "metadata": {
        "id": "9S2Vh0--iucb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords, gutenberg, reuters, webtext, genesis, brown\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Download required resources\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('reuters')\n",
        "nltk.download('webtext')\n",
        "nltk.download('genesis')\n",
        "nltk.download('brown')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKpni5X9hFo7",
        "outputId": "92dc3d58-a3c8-40c5-9e60-8955c21d2ada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Package webtext is already up-to-date!\n",
            "[nltk_data] Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]   Package genesis is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to print corpus statistics\n",
        "def print_corpus_stats(corpus, corpus_name):\n",
        "    words = corpus.words()\n",
        "    sentences = corpus.sents() if hasattr(corpus, 'sents') else None\n",
        "    print(f\"Corpus: {corpus_name}\")\n",
        "    print(f\"Tagged: {'Yes' if hasattr(corpus, 'tagged_words') else 'No'}\")\n",
        "    print(f\"Total Words: {len(words)}\")\n",
        "    if sentences:\n",
        "        print(f\"Total Sentences: {len(sentences)}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# Check and print statistics of the mentioned corpora\n",
        "print_corpus_stats(gutenberg, \"Gutenberg\")\n",
        "print_corpus_stats(reuters, \"Reuters\")\n",
        "print_corpus_stats(webtext, \"Webtext\")\n",
        "print_corpus_stats(genesis, \"Genesis\")\n",
        "print_corpus_stats(brown, \"Brown\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fBv3rj8hSlS",
        "outputId": "024f6e89-4c74-4325-e92c-3d8cf315854f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus: Gutenberg\n",
            "Tagged: No\n",
            "Total Words: 2621613\n",
            "Total Sentences: 98552\n",
            "--------------------------------------------------\n",
            "Corpus: Reuters\n",
            "Tagged: No\n",
            "Total Words: 1720901\n",
            "Total Sentences: 54716\n",
            "--------------------------------------------------\n",
            "Corpus: Webtext\n",
            "Tagged: No\n",
            "Total Words: 396733\n",
            "Total Sentences: 25733\n",
            "--------------------------------------------------\n",
            "Corpus: Genesis\n",
            "Tagged: No\n",
            "Total Words: 315268\n",
            "Total Sentences: 13640\n",
            "--------------------------------------------------\n",
            "Corpus: Brown\n",
            "Tagged: Yes\n",
            "Total Words: 1161192\n",
            "Total Sentences: 57340\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# selected a document from  Gutenberg\n",
        "doc = gutenberg.raw('shakespeare-caesar.txt')\n",
        "print(f\"\\nSelected Document: shakespeare's 'caesar' \")\n",
        "\n",
        "# Tokenization\n",
        "words = word_tokenize(doc)\n",
        "sentences = sent_tokenize(doc)\n",
        "print(f\"Total Words in Document: {len(words)}\")\n",
        "print(f\"Total Sentences in Document: {len(sentences)}\")\n",
        "\n",
        "# Remove stopwords and punctuation\n",
        "stop_words = set(stopwords.words('english'))\n",
        "translator = str.maketrans('', '', string.punctuation)\n",
        "filtered_words = [word.translate(translator) for word in words if word.translate(translator) and word.lower() not in stop_words]\n",
        "\n",
        "# Print unique words\n",
        "unique_words = set(filtered_words)\n",
        "print(f\"\\nNumber of Unique Words (after cleaning): {len(unique_words)}\")\n",
        "print(\"Unique Words:\", list(unique_words)[:20])  # Display first 20 unique words\n",
        "\n",
        "# POS Tagging\n",
        "pos_tags = pos_tag(filtered_words[:50])  # POS tag first 50 words for brevity\n",
        "print(\"\\nPOS Tags (first 50 words):\", pos_tags)\n",
        "\n",
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in unique_words]\n",
        "print(\"\\nStemmed Words (first 20):\", stemmed_words[:20])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFSuZrtGhaN5",
        "outputId": "1df777b8-63ac-46ab-cbb7-7d185e09f3d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Selected Document: shakespeare's 'caesar' \n",
            "Total Words in Document: 25277\n",
            "Total Sentences in Document: 1592\n",
            "\n",
            "Number of Unique Words (after cleaning): 3393\n",
            "Unique Words: ['Ruines', 'seeme', 'curtesie', 'begins', 'Since', 'houses', 'Question', 'Betweene', 'obscurely', 'Without', 'resolu', 'reading', 'Ambitions', 'gentlenesse', 'hence', 'beene', 'fellow', 'Affabilitie', 'Sawcy', 'valiant']\n",
            "\n",
            "POS Tags (first 50 words): [('Tragedie', 'NNP'), ('Julius', 'NNP'), ('Caesar', 'NNP'), ('William', 'NNP'), ('Shakespeare', 'NNP'), ('1599', 'CD'), ('Actus', 'NNP'), ('Primus', 'NNP'), ('Scoena', 'NNP'), ('Prima', 'NNP'), ('Enter', 'NNP'), ('Flauius', 'NNP'), ('Murellus', 'NNP'), ('certaine', 'NN'), ('Commoners', 'NNP'), ('ouer', 'NN'), ('Stage', 'NNP'), ('Flauius', 'NNP'), ('Hence', 'NNP'), ('home', 'NN'), ('idle', 'NN'), ('Creatures', 'NNP'), ('get', 'VBP'), ('home', 'NN'), ('Holiday', 'NNP'), ('know', 'VBP'), ('Mechanicall', 'NNP'), ('ought', 'MD'), ('walke', 'VB'), ('Vpon', 'NNP'), ('labouring', 'VBG'), ('day', 'NN'), ('without', 'IN'), ('signe', 'JJ'), ('Profession', 'NNP'), ('Speake', 'NNP'), ('Trade', 'NNP'), ('art', 'NN'), ('thou', 'NN'), ('Car', 'NNP'), ('Sir', 'NNP'), ('Carpenter', 'NNP'), ('Mur', 'NNP'), ('thy', 'VBZ'), ('Leather', 'NNP'), ('Apron', 'NNP'), ('thy', 'NN'), ('Rule', 'NNP'), ('dost', 'NN'), ('thou', 'NN')]\n",
            "\n",
            "Stemmed Words (first 20): ['ruin', 'seem', 'curtesi', 'begin', 'sinc', 'hous', 'question', 'between', 'obscur', 'without', 'resolu', 'read', 'ambit', 'gentleness', 'henc', 'been', 'fellow', 'affabiliti', 'sawci', 'valiant']\n"
          ]
        }
      ]
    }
  ]
}