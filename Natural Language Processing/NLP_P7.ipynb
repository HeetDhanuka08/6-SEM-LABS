{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "NAME : Heet Dhanuka\n",
        "\n",
        "ROLL NO.: 34\n",
        "\n",
        "BATCH: B2\n",
        "\n",
        "NLP PRACTICAL 7\n",
        "\n",
        "___\n",
        "\n",
        "AIM : Write a python program for handling text data using n gram techniques.Implement Smoothing techniques like laplace ,good turing discounting,back off models to adjust probabilities for unseen n gram."
      ],
      "metadata": {
        "id": "8OqVxaLaiVmr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7xc_BZAW4SM",
        "outputId": "ee1a3922-b6e8-469b-bc52-8bd1d41dae27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigrams: [('the', 'cat'), ('cat', 'sat'), ('sat', 'on'), ('on', 'the'), ('the', 'mat'), ('mat', '.')]\n",
            "Trigrams: [('the', 'cat', 'sat'), ('cat', 'sat', 'on'), ('sat', 'on', 'the'), ('on', 'the', 'mat'), ('the', 'mat', '.')]\n"
          ]
        }
      ],
      "source": [
        "from nltk import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "text = \"The cat sat on the mat.\"\n",
        "\n",
        "# Tokenize and convert to lowercase\n",
        "tokens = word_tokenize(text.lower())\n",
        "\n",
        "# Generate bigrams\n",
        "bigrams = list(ngrams(tokens, 2))\n",
        "print(\"Bigrams:\", bigrams)\n",
        "\n",
        "# Generate trigrams\n",
        "trigrams = list(ngrams(tokens, 3))\n",
        "print(\"Trigrams:\", trigrams)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from collections import defaultdict\n",
        "\n",
        "# Applies Laplace smoothing\n",
        "def laplace_smoothing(ngrams, vocabulary_size, k=1):\n",
        "    ngram_counts = defaultdict(int)\n",
        "    for ngram in ngrams:\n",
        "        ngram_counts[ngram] += 1\n",
        "\n",
        "    probabilities = {}\n",
        "    for ngram, count in ngram_counts.items():\n",
        "      probabilities[ngram] = (count + k) / (sum(ngram_counts.values()) + k * vocabulary_size)\n",
        "\n",
        "    return probabilities\n",
        "\n",
        "# Applies Good-Turing\n",
        "def good_turing_discounting(ngrams):\n",
        "    ngram_counts = defaultdict(int)\n",
        "    for ngram in ngrams:\n",
        "        ngram_counts[ngram] += 1\n",
        "\n",
        "    probabilities = {}\n",
        "    for ngram, count in ngram_counts.items():\n",
        "        if count == 0:\n",
        "            probabilities[ngram] = 0.01\n",
        "        else:\n",
        "            probabilities[ngram] = count / sum(ngram_counts.values())\n",
        "    return probabilities\n",
        "\n",
        "# Applies backoff model\n",
        "def backoff_model(unigrams, bigrams, trigrams):\n",
        "    unigram_probs = laplace_smoothing(unigrams, len(set(unigrams)))\n",
        "    bigram_probs = laplace_smoothing(bigrams, len(set(unigrams)))\n",
        "    trigram_probs = laplace_smoothing(trigrams, len(set(unigrams)))\n",
        "\n",
        "    probabilities = {}\n",
        "    for trigram in trigrams:\n",
        "        if trigram in trigram_probs:\n",
        "            probabilities[trigram] = trigram_probs[trigram]\n",
        "        elif trigram[:2] in bigram_probs:\n",
        "            probabilities[trigram] = 0.4 * bigram_probs[trigram[:2]]\n",
        "        else:\n",
        "            probabilities[trigram] = 0.1 * unigram_probs[trigram[2]]\n",
        "    return probabilities\n",
        "\n",
        "text = \"The cat sat on the mat.\"\n",
        "tokens = word_tokenize(text.lower())\n",
        "unigrams = list(ngrams(tokens,1))\n",
        "bigrams = list(ngrams(tokens, 2))\n",
        "trigrams = list(ngrams(tokens, 3))\n",
        "\n",
        "vocabulary_size = len(set(tokens))\n",
        "\n",
        "# Apply smoothing techniques\n",
        "laplace_probs = laplace_smoothing(bigrams, vocabulary_size)\n",
        "print(\"Laplace Smoothed Probabilities:\", laplace_probs)\n",
        "\n",
        "good_turing_probs = good_turing_discounting(bigrams)\n",
        "print(\"Good Turing Probabilities:\", good_turing_probs)\n",
        "\n",
        "backoff_probs = backoff_model(unigrams,bigrams,trigrams)\n",
        "print(\"Backoff Model Probabilities:\", backoff_probs)\n",
        "\n"
      ],
      "metadata": {
        "id": "Zp1WHyEZaYp5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e39bd7f-eec9-422e-9550-20eb6b553dea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Laplace Smoothed Probabilities: {('the', 'cat'): 0.16666666666666666, ('cat', 'sat'): 0.16666666666666666, ('sat', 'on'): 0.16666666666666666, ('on', 'the'): 0.16666666666666666, ('the', 'mat'): 0.16666666666666666, ('mat', '.'): 0.16666666666666666}\n",
            "Good Turing Probabilities: {('the', 'cat'): 0.16666666666666666, ('cat', 'sat'): 0.16666666666666666, ('sat', 'on'): 0.16666666666666666, ('on', 'the'): 0.16666666666666666, ('the', 'mat'): 0.16666666666666666, ('mat', '.'): 0.16666666666666666}\n",
            "Backoff Model Probabilities: {('the', 'cat', 'sat'): 0.18181818181818182, ('cat', 'sat', 'on'): 0.18181818181818182, ('sat', 'on', 'the'): 0.18181818181818182, ('on', 'the', 'mat'): 0.18181818181818182, ('the', 'mat', '.'): 0.18181818181818182}\n"
          ]
        }
      ]
    }
  ]
}