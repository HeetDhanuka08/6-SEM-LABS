{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "NAME : Heet Dhanuka\n",
        "\n",
        "ROLL NO.: 34\n",
        "\n",
        "BATCH: B2\n",
        "\n",
        "NLP PRACTICAL 8\n",
        "\n",
        "___\n",
        "\n",
        "AIM : Write a python program based on supervised disambiguition using\n",
        "baysian classification and dictionary based disambiguition using lesk algorithm."
      ],
      "metadata": {
        "id": "joH5lplxaE5X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yEqTlrQ1sxly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk import word_tokenize\n",
        "from collections import Counter\n",
        "import random\n",
        "import numpy as np\n",
        "from nltk.classify import NaiveBayesClassifier"
      ],
      "metadata": {
        "id": "X5Pq_W_4fwnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlthKkZkiq_L",
        "outputId": "89ab4e18-2655-4325-eb70-02f5df198c01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(context):\n",
        "  words = set(word_tokenize(context.lower()))\n",
        "  return {word: (word in words) for word in words}"
      ],
      "metadata": {
        "id": "QE-sev08iuzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_training_data():\n",
        "  training_data = []\n",
        "  for synset in wn.all_synsets():\n",
        "    for lemma in synset.lemmas():\n",
        "      example_sentences = synset.examples()\n",
        "      for example in example_sentences:\n",
        "        features = extract_features(example)\n",
        "        training_data.append((features, synset.name()))\n",
        "  return training_data"
      ],
      "metadata": {
        "id": "1uCO9I1jizqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = create_training_data()\n",
        "training_data = training_data[:10000]\n",
        "print(\"Number of training data points:\", len(training_data))\n",
        "random.shuffle(training_data) # Shuffle for randomness\n",
        "train_size = int(0.8 * len(training_data)) # 80% for training\n",
        "train_set, test_set = training_data[:train_size], training_data[train_size:]\n",
        "classifier = NaiveBayesClassifier.train(train_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmc7gpecjPY9",
        "outputId": "9ca5aeb8-fa8c-4f29-9512-33e89757eb9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training data points: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def bayesian_wsd(word, context):\n",
        "  features = extract_features(context)\n",
        "  return classifier.classify(features)"
      ],
      "metadata": {
        "id": "Ku2LqYwVjVIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_sentence = \"He went to the bank to withdraw money.\"\n",
        "print(bayesian_wsd(\"bank\", context_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gE24e-6vluQb",
        "outputId": "fb9faf7d-1f0d-428e-f20b-9ee9f5ee5bfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "courteous.a.02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lesk_wsd(word, sentence):\n",
        "  best_sense = None\n",
        "  max_overlap = 0\n",
        "  context = set(word_tokenize(sentence.lower()))\n",
        "  for sense in wn.synsets(word):  # Get all possible meanings of the word\n",
        "    signature = set(word_tokenize(sense.definition().lower()))  # Definition words\n",
        "    for example in sense.examples():  # Add example words\n",
        "      signature.update(word_tokenize(example.lower()))\n",
        "    overlap = len(signature.intersection(context))  # Count common words\n",
        "    if overlap > max_overlap:\n",
        "      max_overlap = overlap\n",
        "      best_sense = sense\n",
        "  return best_sense.name() if best_sense else None\n",
        "\n",
        "\n",
        "context_sentence = \"He is writing  a book\"\n",
        "print(lesk_wsd(\"book\", context_sentence))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWCJd_5ElzQS",
        "outputId": "4eac6f3d-fd24-4010-bc68-f4528ddda325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "book.n.02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "09az3F0tpdQ0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}