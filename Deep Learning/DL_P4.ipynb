{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Name :- Heet Dhanuka\n",
        "\n",
        "Batch :- B2\n",
        "\n",
        "Roll No. :- 34\n",
        "\n",
        "DL practical 4\n",
        "\n",
        "---------\n",
        "\n",
        "Aim :- Create a neural network from scratch for a multiclass classification\n",
        "task with the following architecture:\n",
        "\n",
        "Input layer: 4 neurons\n",
        "First hidden layer: 3 neurons\n",
        "Second hidden layer: 4 neurons\n",
        "Output layer: 3 neurons\n",
        "\n",
        "Generate a random dataset for a 3-class classification problem. Apply\n",
        "the designed neural network on generated dataset using the ReLU activation\n",
        "function in the hidden layers and the softmax activation function in the output\n",
        "layer for multiclass classification. Use categorical cross-entropy as the loss\n",
        "function for training. Additionally, implement the Gradient Descent, Momentum\n",
        "based GD, NAG, AdaGrad, RMS prop, Adam optimizer as part of the training\n",
        "process. Train the neural network using all optimizers and evaluate their\n",
        "performance on the dataset using performance metrics accuracy."
      ],
      "metadata": {
        "id": "GlvVEMZBqEaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import library\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "metadata": {
        "id": "YpVsdhlKqRff"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate random dataset\n",
        "num_samples = 1000\n",
        "num_features = 4\n",
        "num_classes = 3\n",
        "\n",
        "# Generate random features (X) and random labels (y) for classification\n",
        "X = np.random.rand(num_samples, num_features)  # Random features\n",
        "y = np.random.randint(0, num_classes, num_samples)  # Random class labels (0, 1, 2)\n",
        "y_onehot = np.eye(num_classes)[y]  # One-hot encode labels"
      ],
      "metadata": {
        "id": "vfCRelGaqTj4"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train/test sets (80/20 split)\n",
        "train_size = int(0.8 * num_samples)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y_onehot[:train_size], y_onehot[train_size:]"
      ],
      "metadata": {
        "id": "ozYvhf1MqbBG"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining Activation function\n",
        "# relu function\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "# relu derivative function\n",
        "def relu_derivative(z):\n",
        "    return (z > 0).astype(float)\n",
        "\n",
        "# SoftMax function\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "# loss function\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    return -np.mean(np.sum(y_true * np.log(y_pred + 1e-8), axis=1))"
      ],
      "metadata": {
        "id": "CVL65v4sqeuQ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy function :\n",
        "def accuracy(y_true, y_pred):\n",
        "    return np.mean(np.argmax(y_true, axis=1) == np.argmax(y_pred, axis=1))"
      ],
      "metadata": {
        "id": "sJ1i3LvArPdu"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize network parameters\n",
        "def initialize_parameters():\n",
        "    np.random.seed(42)\n",
        "    params = {\n",
        "        \"W1\": np.random.randn(4, 3) * 0.01,\n",
        "        \"b1\": np.zeros((1, 3)),\n",
        "        \"W2\": np.random.randn(3, 4) * 0.01,\n",
        "        \"b2\": np.zeros((1, 4)),\n",
        "        \"W3\": np.random.randn(4, 3) * 0.01,\n",
        "        \"b3\": np.zeros((1, 3)),\n",
        "    }\n",
        "    return params"
      ],
      "metadata": {
        "id": "uVuNGv8wrQdX"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward and backward propagation\n",
        "def forward_propagation(X, params):\n",
        "    Z1 = np.dot(X, params[\"W1\"]) + params[\"b1\"]\n",
        "    A1 = relu(Z1)\n",
        "    Z2 = np.dot(A1, params[\"W2\"]) + params[\"b2\"]\n",
        "    A2 = relu(Z2)\n",
        "    Z3 = np.dot(A2, params[\"W3\"]) + params[\"b3\"]\n",
        "    A3 = softmax(Z3)\n",
        "    cache = {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2, \"Z3\": Z3, \"A3\": A3}\n",
        "    return A3, cache"
      ],
      "metadata": {
        "id": "R0-MSykHrSrD"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_propagation(X, y, params, cache):\n",
        "    m = X.shape[0]\n",
        "\n",
        "    dZ3 = cache[\"A3\"] - y\n",
        "    dW3 = np.dot(cache[\"A2\"].T, dZ3) / m\n",
        "    db3 = np.sum(dZ3, axis=0, keepdims=True) / m\n",
        "\n",
        "    dA2 = np.dot(dZ3, params[\"W3\"].T)\n",
        "    dZ2 = dA2 * relu_derivative(cache[\"Z2\"])\n",
        "    dW2 = np.dot(cache[\"A1\"].T, dZ2) / m\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
        "\n",
        "    dA1 = np.dot(dZ2, params[\"W2\"].T)\n",
        "    dZ1 = dA1 * relu_derivative(cache[\"Z1\"])\n",
        "    dW1 = np.dot(X.T, dZ1) / m\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
        "\n",
        "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2, \"dW3\": dW3, \"db3\": db3}\n",
        "    return grads"
      ],
      "metadata": {
        "id": "-L6I7c-MrUsa"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Descent Optimizer\n",
        "def gradient_descent(params, grads, learning_rate):\n",
        "    for key in params.keys():\n",
        "        params[key] -= learning_rate * grads[\"d\" + key]\n",
        "    return params"
      ],
      "metadata": {
        "id": "8HCqezvTrWZn"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Momentum based GD\n",
        "def momentum_gd(params, grads, learning_rate, momentum=0.9, v=None):\n",
        "  if v is None:\n",
        "    v = {key: np.zeros_like(params[key]) for key in params}\n",
        "  for key in params.keys():\n",
        "      v[key] = momentum * v[key] + learning_rate * grads[\"d\" + key]\n",
        "      params[key] -= v[key]\n",
        "  return params, v"
      ],
      "metadata": {
        "id": "lV0XbGePuzLl"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NAG optimizer\n",
        "def nag(params, grads, learning_rate, momentum=0.9, v=None):\n",
        "    if v is None:\n",
        "        v = {key: np.zeros_like(params[key]) for key in params}\n",
        "    for key in params.keys():\n",
        "        v[key] = momentum * v[key] + learning_rate * grads[\"d\" + key]\n",
        "        params[key] -= v[key]\n",
        "    return params, v\n",
        "\n"
      ],
      "metadata": {
        "id": "s4qQngHquzID"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AdaGrad optimizer\n",
        "def adagrad(params, grads, learning_rate, epsilon=1e-8, cache=None):\n",
        "    if cache is None:\n",
        "        cache = {key: np.zeros_like(params[key]) for key in params}\n",
        "    for key in params.keys():\n",
        "        cache[key] += grads[\"d\" + key]**2\n",
        "        params[key] -= (learning_rate / np.sqrt(cache[key] + epsilon)) * grads[\"d\" + key]\n",
        "    return params, cache\n",
        "\n"
      ],
      "metadata": {
        "id": "35Ot17MWuzFM"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RMSprop optimizer\n",
        "def rmsprop(params, grads, learning_rate, decay_rate=0.9, epsilon=1e-8, cache=None):\n",
        "    if cache is None:\n",
        "        cache = {key: np.zeros_like(params[key]) for key in params}\n",
        "    for key in params.keys():\n",
        "        cache[key] = decay_rate * cache[key] + (1 - decay_rate) * grads[\"d\" + key]**2\n",
        "        params[key] -= (learning_rate / np.sqrt(cache[key] + epsilon)) * grads[\"d\" + key]\n",
        "    return params, cache\n",
        "\n"
      ],
      "metadata": {
        "id": "zBoN5eO7uzCR"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adam optimizer\n",
        "def adam(params, grads, learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8, m=None, v=None, t=0):\n",
        "    if m is None:\n",
        "        m = {key: np.zeros_like(params[key]) for key in params}\n",
        "    if v is None:\n",
        "        v = {key: np.zeros_like(params[key]) for key in params}\n",
        "    t += 1\n",
        "    for key in params.keys():\n",
        "        m[key] = beta1 * m[key] + (1 - beta1) * grads[\"d\" + key]\n",
        "        v[key] = beta2 * v[key] + (1 - beta2) * grads[\"d\" + key]**2\n",
        "\n",
        "        m_hat = m[key] / (1 - beta1**t)\n",
        "        v_hat = v[key] / (1 - beta2**t)\n",
        "\n",
        "        params[key] -= (learning_rate / np.sqrt(v_hat + epsilon)) * m_hat\n",
        "    return params, m, v, t"
      ],
      "metadata": {
        "id": "AaJ95Cmcuy_F"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizers = {\n",
        "    \"GD\": gradient_descent,\n",
        "    \"Momentum\": momentum_gd,\n",
        "    \"NAG\": nag,\n",
        "    \"AdaGrad\": adagrad,\n",
        "    \"RMSprop\": rmsprop,\n",
        "    \"Adam\": adam\n",
        "}\n",
        "\n",
        "for optimizer_name, optimizer_func in optimizers.items():\n",
        "    print(f\"\\nTraining with {optimizer_name}:\")\n",
        "    trained_params = train(X_train, y_train, optimizer_func, learning_rate=0.01, epochs=100)\n",
        "\n",
        "    A3_test, _ = forward_propagation(X_test, trained_params)\n",
        "    test_accuracy = accuracy(y_test, A3_test)\n",
        "    print(f\"Test Accuracy ({optimizer_name}): {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "x6qtyO7UvNkU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7570cded-f02d-4855-9296-bfe3145836bb"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with GD:\n",
            "Epoch 10/100, Loss: 1.0986, Accuracy: 0.3450\n",
            "Epoch 20/100, Loss: 1.0986, Accuracy: 0.3450\n",
            "Epoch 30/100, Loss: 1.0986, Accuracy: 0.3450\n",
            "Epoch 40/100, Loss: 1.0985, Accuracy: 0.3450\n",
            "Epoch 50/100, Loss: 1.0985, Accuracy: 0.3450\n",
            "Epoch 60/100, Loss: 1.0985, Accuracy: 0.3450\n",
            "Epoch 70/100, Loss: 1.0985, Accuracy: 0.3450\n",
            "Epoch 80/100, Loss: 1.0985, Accuracy: 0.3450\n",
            "Epoch 90/100, Loss: 1.0985, Accuracy: 0.3450\n",
            "Epoch 100/100, Loss: 1.0984, Accuracy: 0.3450\n",
            "Test Accuracy (GD): 0.3800\n",
            "\n",
            "Training with Momentum:\n",
            "Epoch 10/100, Loss: 1.0985, Accuracy: 0.3450\n",
            "Epoch 20/100, Loss: 1.0984, Accuracy: 0.3450\n",
            "Epoch 30/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Epoch 40/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Epoch 50/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Epoch 60/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Epoch 70/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Epoch 80/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Epoch 90/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Epoch 100/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Test Accuracy (Momentum): 0.3800\n",
            "\n",
            "Training with NAG:\n",
            "Epoch 10/100, Loss: 1.0985, Accuracy: 0.3450\n",
            "Epoch 20/100, Loss: 1.0984, Accuracy: 0.3450\n",
            "Epoch 30/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Epoch 40/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Epoch 50/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Epoch 60/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Epoch 70/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Epoch 80/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Epoch 90/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Epoch 100/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Test Accuracy (NAG): 0.3800\n",
            "\n",
            "Training with AdaGrad:\n",
            "Epoch 10/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Epoch 20/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Epoch 30/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Epoch 40/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Epoch 50/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Epoch 60/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Epoch 70/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Epoch 80/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Epoch 90/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Epoch 100/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Test Accuracy (AdaGrad): 0.3800\n",
            "\n",
            "Training with RMSprop:\n",
            "Epoch 10/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Epoch 20/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Epoch 30/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Epoch 40/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Epoch 50/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Epoch 60/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Epoch 70/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Epoch 80/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Epoch 90/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Epoch 100/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Test Accuracy (RMSprop): 0.3800\n",
            "\n",
            "Training with Adam:\n",
            "Epoch 10/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Epoch 20/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Epoch 30/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Epoch 40/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Epoch 50/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Epoch 60/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Epoch 70/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Epoch 80/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Epoch 90/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Epoch 100/100, Loss: 1.0983, Accuracy: 0.3450\n",
            "Test Accuracy (Adam): 0.3800\n"
          ]
        }
      ]
    }
  ]
}